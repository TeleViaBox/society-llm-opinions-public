{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d81ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "\n",
    "# 設定環境變量和檔案路徑\n",
    "open_ai_key_path = \"./open-ai-key.txt\"\n",
    "\n",
    "with open(open_ai_key_path, \"r\") as file:\n",
    "    open_ai_key = file.readline().strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9431eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Chroma database from ./chroma_db\n"
     ]
    }
   ],
   "source": [
    "class Chroma:\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        # Placeholder for actual loading logic\n",
    "        print(f\"Loading Chroma database from {path}\")\n",
    "        return Chroma()\n",
    "\n",
    "# Now trying to load the Chroma database\n",
    "vectordb = Chroma.load('./chroma_db')\n",
    "\n",
    "# # 加載現有的Chroma資料庫\n",
    "# vectordb = Chroma.load('./chroma_db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfe76ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.vectorstores import Chroma as LangChainChroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "\n",
    "# Set environment variables and file paths\n",
    "open_ai_key_path = \"./open-ai-key.txt\"\n",
    "\n",
    "with open(open_ai_key_path, \"r\") as file:\n",
    "    open_ai_key = file.readline().strip()\n",
    "\n",
    "# Define the embedding function\n",
    "embedding_function = OpenAIEmbeddings(openai_api_key=open_ai_key, model=\"text-embedding-ada-002\")  # Ensure consistent model\n",
    "\n",
    "# Load the Chroma database from the specified path with consistent embedding function\n",
    "def load_chroma_db(path):\n",
    "    return LangChainChroma(persist_directory=path, embedding_function=embedding_function)\n",
    "\n",
    "# Load the Chroma database\n",
    "vectordb = load_chroma_db('./chroma_db')\n",
    "\n",
    "def get_answer(query):\n",
    "    new_line = '\\n'\n",
    "    template = f\"Use the following pieces of context to answer truthfully.{new_line}If the context does not provide the truthful answer, make the answer as truthful as possible.{new_line}Use 15 words maximum. Keep the response as concise as possible.{new_line}{{context}}{new_line}Question: {{question}}{new_line}Response: \"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "\n",
    "    llm = OpenAI(openai_api_key=open_ai_key)\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectordb.as_retriever(), return_source_documents=True, chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "\n",
    "#     result = qa_chain({\"query\": query})\n",
    "#     return result[\"result\"]\n",
    "\n",
    "\n",
    "    # Debug: Check the retrieved documents\n",
    "    retrieved_docs = vectordb.similarity_search(query, k=3)\n",
    "    print(\"Retrieved Documents:\", retrieved_docs)\n",
    "\n",
    "    result = qa_chain({\"query\": query})\n",
    "    return result[\"result\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e59c394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your question (type 'exit' to quit): hello\n",
      "Retrieved Documents: [Document(page_content='Introduction', metadata={'source': './novelsascii\\\\11wizard.txt'}), Document(page_content='question, dear??', metadata={'source': './novelsascii\\\\6Little-Women.txt'}), Document(page_content='* * * * *', metadata={'source': './novelsascii\\\\2christmas.txt'})]\n",
      "Answer:  Hi! How can I help you?\n",
      "Please enter your question (type 'exit' to quit): What caused Alice to start shrinking in size?\n",
      "Retrieved Documents: [Document(page_content='growing, and she was quite surprised to find that she remained the same\\nsize: to be sure, this generally happens when one eats cake, but Alice', metadata={'source': './novelsascii\\\\1Adventures.txt'}), Document(page_content='?I wish you wouldn?t squeeze so.? said the Dormouse, who was sitting\\nnext to her. ?I can hardly breathe.?\\n\\n?I can?t help it,? said Alice very meekly: ?I?m growing.?', metadata={'source': './novelsascii\\\\1Adventures.txt'}), Document(page_content='?I?I?m a little girl,? said Alice, rather doubtfully, as she remembered\\nthe number of changes she had gone through that day.', metadata={'source': './novelsascii\\\\1Adventures.txt'})]\n",
      "Answer:  Eating cake caused Alice to shrink.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    input_text = input(\"Please enter your question (type 'exit' to quit): \")\n",
    "    if input_text.lower() == 'exit':\n",
    "        break\n",
    "    response = get_answer(input_text)\n",
    "    print(\"Answer:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
