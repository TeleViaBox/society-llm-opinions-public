{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d81ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "\n",
    "# 設定環境變量和檔案路徑\n",
    "open_ai_key_path = \"./open-ai-key.txt\"\n",
    "\n",
    "with open(open_ai_key_path, \"r\") as file:\n",
    "    open_ai_key = file.readline().strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a9431eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chroma:\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        # Placeholder for actual loading logic\n",
    "        print(f\"Loading Chroma database from {path}\")\n",
    "        return Chroma()\n",
    "\n",
    "# Now trying to load the Chroma database\n",
    "vectordb = Chroma.load('./chroma_db')\n",
    "\n",
    "# # 加載現有的Chroma資料庫\n",
    "# vectordb = Chroma.load('./chroma_db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb36d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_answer(query):\n",
    "    new_line = '\\n'\n",
    "    template = f\"Use the following pieces of context to answer truthfully.{new_line}If the context does not provide the truthful answer, make the answer as truthful as possible.{new_line}Use 15 words maximum. Keep the response as concise as possible.{new_line}{{context}}{new_line}Question: {{question}}{new_line}Response: \"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "\n",
    "    llm = OpenAI(openai_api_key=open_ai_key)\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                           retriever=vectordb.as_retriever(),\n",
    "                                           return_source_documents=True,\n",
    "                                           chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "\n",
    "    result = qa_chain({\"query\": query})\n",
    "    return result[\"result\"]\n",
    "\n",
    "# app = Flask(__name__)\n",
    "\n",
    "# @app.route('/')\n",
    "# def index():\n",
    "#     return render_template('index.html')\n",
    "\n",
    "# @app.route('/chat', methods=['POST'])\n",
    "# def chat():\n",
    "#     data = request.json\n",
    "#     input_text = data['message']\n",
    "#     response = get_answer(input_text)\n",
    "#     return jsonify({\"response\": response})\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a507bf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your question (type 'exit' to quit): hello\n"
     ]
    }
   ],
   "source": [
    "# 终端交互循环\n",
    "while True:\n",
    "    input_text = input(\"Please enter your question (type 'exit' to quit): \")\n",
    "    if input_text.lower() == 'exit':\n",
    "        break\n",
    "    response = get_answer(input_text)\n",
    "    print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "043a9185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with watchdog (windowsapi)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "\n",
    "# 設定環境變量和檔案路徑\n",
    "open_ai_key_path = \"./open-ai-key.txt\"\n",
    "\n",
    "with open(open_ai_key_path, \"r\") as file:\n",
    "    open_ai_key = file.readline().strip()\n",
    "\n",
    "\n",
    "class Chroma:\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        # Placeholder for actual loading logic\n",
    "        print(f\"Loading Chroma database from {path}\")\n",
    "        return Chroma()\n",
    "\n",
    "# Now trying to load the Chroma database\n",
    "vectordb = Chroma.load('./chroma_db')\n",
    "\n",
    "# # 加載現有的Chroma資料庫\n",
    "# vectordb = Chroma.load('./chroma_db')\n",
    "\n",
    "def get_answer(query):\n",
    "    new_line = '\\n'\n",
    "    template = f\"Use the following pieces of context to answer truthfully.{new_line}If the context does not provide the truthful answer, make the answer as truthful as possible.{new_line}Use 15 words maximum. Keep the response as concise as possible.{new_line}{{context}}{new_line}Question: {{question}}{new_line}Response: \"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "\n",
    "    llm = OpenAI(openai_api_key=open_ai_key)\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                           retriever=vectordb.as_retriever(),\n",
    "                                           return_source_documents=True,\n",
    "                                           chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "\n",
    "    result = qa_chain({\"query\": query})\n",
    "    return result[\"result\"]\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    data = request.json\n",
    "    input_text = data['message']\n",
    "    response = get_answer(input_text)\n",
    "    return jsonify({\"response\": response})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2ef9ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Chroma database from ./chroma_db\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.vectorstores import Chroma as LangChainChroma  # Import with alias to avoid conflict\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "\n",
    "# 設定環境變量和檔案路徑\n",
    "open_ai_key_path = \"./open-ai-key.txt\"\n",
    "\n",
    "with open(open_ai_key_path, \"r\") as file:\n",
    "    open_ai_key = file.readline().strip()\n",
    "\n",
    "class ChromaDB:\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        # Load the Chroma database from the specified path\n",
    "        # This should return an instance of LangChainChroma\n",
    "        # Replace the following line with actual logic to load and return LangChainChroma instance\n",
    "        print(f\"Loading Chroma database from {path}\")\n",
    "        return LangChainChroma()  # Placeholder, replace with actual loading logic\n",
    "\n",
    "# Now trying to load the Chroma database\n",
    "vectordb = ChromaDB.load('./chroma_db')\n",
    "\n",
    "def get_answer(query):\n",
    "    new_line = '\\n'\n",
    "    template = f\"Use the following pieces of context to answer truthfully.{new_line}If the context does not provide the truthful answer, make the answer as truthful as possible.{new_line}Use 15 words maximum. Keep the response as concise as possible.{new_line}{{context}}{new_line}Question: {{question}}{new_line}Response: \"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "\n",
    "    llm = OpenAI(openai_api_key=open_ai_key)\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                           retriever=vectordb.as_retriever(),  # Make sure this uses the correct Chroma instance\n",
    "                                           return_source_documents=True,\n",
    "                                           chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "\n",
    "    result = qa_chain({\"query\": query})\n",
    "    return result[\"result\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e59c394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your question (type 'exit' to quit): hello\n",
      "Retrieved Documents: [Document(page_content='Introduction', metadata={'source': './novelsascii\\\\11wizard.txt'}), Document(page_content='question, dear??', metadata={'source': './novelsascii\\\\6Little-Women.txt'}), Document(page_content='* * * * *', metadata={'source': './novelsascii\\\\2christmas.txt'})]\n",
      "Answer:  Hi! How can I help you?\n",
      "Please enter your question (type 'exit' to quit): What caused Alice to start shrinking in size?\n",
      "Retrieved Documents: [Document(page_content='growing, and she was quite surprised to find that she remained the same\\nsize: to be sure, this generally happens when one eats cake, but Alice', metadata={'source': './novelsascii\\\\1Adventures.txt'}), Document(page_content='?I wish you wouldn?t squeeze so.? said the Dormouse, who was sitting\\nnext to her. ?I can hardly breathe.?\\n\\n?I can?t help it,? said Alice very meekly: ?I?m growing.?', metadata={'source': './novelsascii\\\\1Adventures.txt'}), Document(page_content='?I?I?m a little girl,? said Alice, rather doubtfully, as she remembered\\nthe number of changes she had gone through that day.', metadata={'source': './novelsascii\\\\1Adventures.txt'})]\n",
      "Answer:  Eating cake.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 终端交互循环\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 3\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease enter your question (type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to quit): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_text\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1175\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1174\u001b[0m     )\n\u001b[1;32m-> 1175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1176\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1179\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1180\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1217\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1216\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# 终端交互循环\n",
    "while True:\n",
    "    input_text = input(\"Please enter your question (type 'exit' to quit): \")\n",
    "    if input_text.lower() == 'exit':\n",
    "        break\n",
    "    response = get_answer(input_text)\n",
    "    print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd8f7e96",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     17\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat caused Alice to start shrinking in size?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 18\u001b[0m result \u001b[38;5;241m=\u001b[39m get_answer(query)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion:\u001b[39m\u001b[38;5;124m\"\u001b[39m, query)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mstrip())\n",
      "Cell \u001b[1;32mIn[22], line 8\u001b[0m, in \u001b[0;36mget_answer\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RetrievalQA\n\u001b[0;32m      7\u001b[0m question \u001b[38;5;241m=\u001b[39m query\n\u001b[1;32m----> 8\u001b[0m qa_chain \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(llm,\n\u001b[0;32m      9\u001b[0m                                       retriever\u001b[38;5;241m=\u001b[39mvectordb\u001b[38;5;241m.\u001b[39mas_retriever(),\n\u001b[0;32m     10\u001b[0m                                       return_source_documents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m                                       chain_type_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: QA_CHAIN_PROMPT})\n\u001b[0;32m     14\u001b[0m result \u001b[38;5;241m=\u001b[39m qa_chain({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: question})\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "    def get_answer(query):\n",
    "        new_line = '\\n'\n",
    "        template = f\"Use the following pieces of context to answer truthfully.{new_line}If the context does not provide the truthful answer, make the answer as truthful as possible.{new_line}Use 15 words maximum. Keep the response as concise as possible.{new_line}{{context}}{new_line}Question: {{question}}{new_line}Response: \"\n",
    "        QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)\n",
    "\n",
    "        from langchain.chains import RetrievalQA\n",
    "        question = query\n",
    "        qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                              retriever=vectordb.as_retriever(),\n",
    "                                              return_source_documents=True,\n",
    "                                              chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "\n",
    "\n",
    "        result = qa_chain({\"query\": question})\n",
    "        return result[\"result\"]\n",
    "\n",
    "    query = \"What caused Alice to start shrinking in size?\"\n",
    "    result = get_answer(query)\n",
    "    print(\"Question:\", query)\n",
    "    print(\"Answer:\", result.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c434f69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1:\n",
      "?Soo?oop of the e?e?evening,\n",
      "    Beautiful, beautiful Soup!?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CHAPTER XI.\n",
      "Who Stole the Tarts?\n",
      "\n",
      "\n",
      "Rank 2:\n",
      "?The Queen of Hearts, she made some tarts,\n",
      "    All on a summer day:\n",
      "The Knave of Hearts, he stole those tarts,\n",
      "    And took them quite away!?\n",
      "\n",
      "\n",
      "?Consider your verdict,? the King said to the jury.\n",
      "\n",
      "\n",
      "Rank 3:\n",
      "nearly out of sight, he said in a deep voice, ?What are tarts made of??\n",
      "\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "query = \"Who Stole the Tarts\"\n",
    "docs = vectordb.similarity_search(query, k=3)\n",
    "for rank, doc in enumerate(docs):\n",
    "    print(f\"Rank {rank+1}:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"\\n\")\n",
    "\n",
    "try:\n",
    "    with open(\"./open-ai-key.txt\", \"r\") as file:\n",
    "        open_ai_key = file.readline().strip()\n",
    "    success = True\n",
    "except FileNotFoundError:\n",
    "    open_ai_key = \"\"\n",
    "    success = False\n",
    "print(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae2a2cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.vectorstores import Chroma as LangChainChroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "\n",
    "# Set environment variables and file paths\n",
    "open_ai_key_path = \"./open-ai-key.txt\"\n",
    "\n",
    "with open(open_ai_key_path, \"r\") as file:\n",
    "    open_ai_key = file.readline().strip()\n",
    "\n",
    "# Load the Chroma database from the specified path\n",
    "def load_chroma_db(path):\n",
    "    embedding_function = OpenAIEmbeddings(openai_api_key=open_ai_key)\n",
    "    return LangChainChroma(persist_directory=path, embedding_function=embedding_function)\n",
    "\n",
    "# Load the Chroma database\n",
    "vectordb = load_chroma_db('./chroma_db')\n",
    "\n",
    "def get_answer(query):\n",
    "    new_line = '\\n'\n",
    "    template = f\"Use the following pieces of context to answer truthfully.{new_line}If the context does not provide the truthful answer, make the answer as truthful as possible.{new_line}Use 15 words maximum. Keep the response as concise as possible.{new_line}{{context}}{new_line}Question: {{question}}{new_line}Response: \"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "\n",
    "    llm = OpenAI(openai_api_key=open_ai_key)\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectordb.as_retriever(), return_source_documents=True, chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "\n",
    "    result = qa_chain({\"query\": query})\n",
    "    return result[\"result\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfe76ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.vectorstores import Chroma as LangChainChroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "\n",
    "# Set environment variables and file paths\n",
    "open_ai_key_path = \"./open-ai-key.txt\"\n",
    "\n",
    "with open(open_ai_key_path, \"r\") as file:\n",
    "    open_ai_key = file.readline().strip()\n",
    "\n",
    "# Define the embedding function\n",
    "embedding_function = OpenAIEmbeddings(openai_api_key=open_ai_key, model=\"text-embedding-ada-002\")  # Ensure consistent model\n",
    "\n",
    "# Load the Chroma database from the specified path with consistent embedding function\n",
    "def load_chroma_db(path):\n",
    "    return LangChainChroma(persist_directory=path, embedding_function=embedding_function)\n",
    "\n",
    "# Load the Chroma database\n",
    "vectordb = load_chroma_db('./chroma_db')\n",
    "\n",
    "def get_answer(query):\n",
    "    new_line = '\\n'\n",
    "    template = f\"Use the following pieces of context to answer truthfully.{new_line}If the context does not provide the truthful answer, make the answer as truthful as possible.{new_line}Use 15 words maximum. Keep the response as concise as possible.{new_line}{{context}}{new_line}Question: {{question}}{new_line}Response: \"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "\n",
    "    llm = OpenAI(openai_api_key=open_ai_key)\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectordb.as_retriever(), return_source_documents=True, chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "\n",
    "#     result = qa_chain({\"query\": query})\n",
    "#     return result[\"result\"]\n",
    "\n",
    "\n",
    "    # Debug: Check the retrieved documents\n",
    "    retrieved_docs = vectordb.similarity_search(query, k=3)\n",
    "    print(\"Retrieved Documents:\", retrieved_docs)\n",
    "\n",
    "    result = qa_chain({\"query\": query})\n",
    "    return result[\"result\"]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
